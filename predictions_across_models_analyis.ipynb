{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covEcho = pd.read_csv(\"./covEcho/predictions_covEcho_class_count_video_lvl_COVID.csv\", index_col=0)\n",
    "\n",
    "# Append \"covEcho\" to all columns except y to identify\n",
    "covEcho.columns = [\"covEcho_\" + col if col != \"y\" else col for col in covEcho.columns]\n",
    "covEcho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = pd.read_csv(\"./Roy_segmentation/predictions_segmentation_class_area_video_lvl_COVID.csv\", index_col=0)\n",
    "\n",
    "# Append \"segmentation\" to all columns except y to identify\n",
    "segmentation.columns = [\"segmentation_\" + col if col != \"y\" else col for col in segmentation.columns]\n",
    "segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two data frames\n",
    "merged = pd.merge(covEcho, segmentation, on=[\"video_name\", \"y\"], how=\"inner\")\n",
    "error_cols = [col for col in merged.columns if \"error\" in col]\n",
    "# Keep only columns containing \"error\" or \"y\"\n",
    "merged_error = merged[[\"y\"] + error_cols]\n",
    "#merged_error = merged_error.apply(lambda x: abs(x))\n",
    "# Sum up all error values for each row\n",
    "merged_error[\"sum\"] = merged_error[error_cols].sum(axis=1)\n",
    "merged_error[\"sum_abs\"] = abs(merged_error[error_cols]).sum(axis=1)\n",
    "\n",
    "# Sort by sum\n",
    "merged_error = merged_error.sort_values(by=\"sum\", ascending=True)\n",
    "merged_error.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, \"./utils/\")\n",
    "from data_utils import get_data_location, get_clinical_df, get_bluepoints_df, get_manual_severity_scores\n",
    "from ml_pipeline import highlight_max\n",
    "from statistics import evaluate_logits\n",
    "\n",
    "DATA_PATH = get_data_location()\n",
    "print(\"Infered Data path:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical data\n",
    "clinical_data = get_clinical_df()\n",
    "\n",
    "clinical_data = clinical_data[[\"Video ID\", \"clin_diagn#COVID19_pneumonia\", \"adm_date\"]]\n",
    "clinical_data.columns = [\"Patient ID\", \"COVID19\", \"adm_date\"]\n",
    "# Transform to datetime. Format is dd-mm-yyyy\n",
    "clinical_data[\"adm_date\"] = pd.to_datetime(clinical_data[\"adm_date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "# Bluepoint data\n",
    "bp = get_bluepoints_df()\n",
    "\n",
    "df = pd.merge(bp, clinical_data, on=\"Patient ID\").sort_values(by=[\"Patient ID\", \"Bluepoint\"])\n",
    "# Reorder columns\n",
    "df = df[[\"Patient ID\", \"Bluepoint\", \"video_name\", \"COVID19\", \"adm_date\"]]\n",
    "\n",
    "# Manual severity scores\n",
    "severity_manual = get_manual_severity_scores()\n",
    "severity_manual = severity_manual[[\"Severity Score\", \"video_name\", \"Tablet\"]]\n",
    "df = pd.merge(df, severity_manual, on=[\"video_name\"])\n",
    "\n",
    "# Merge with the predictions\n",
    "df = pd.merge(df, merged_error, on=\"video_name\")\n",
    "\n",
    "df.sort_values(by=[\"sum\", \"Patient ID\"], ascending=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sum of errors \n",
    "sns.set()\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set_context(\"paper\", font_scale=1.5)\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.barplot(x=\"video_name\", y=\"sum\", data=df, hue=\"Tablet\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Video name\")\n",
    "# Decrease x label size\n",
    "plt.xticks(fontsize=4)\n",
    "plt.ylabel(\"Sum of errors\")\n",
    "plt.title(\"Sum of errors for each video\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"sum_of_errors.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statitical analysis Tablet A vs Tablet C\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "Tabelt_A = df[df[\"Tablet\"] == \"A\"]\n",
    "Tabelt_C = df[df[\"Tablet\"] == \"C\"]\n",
    "\n",
    "stat, p = ttest_ind(Tabelt_A[\"sum\"], Tabelt_C[\"sum\"])\n",
    "print(\"Mean sum of errors for Tablet A:\", Tabelt_A[\"sum\"].mean())\n",
    "print(\"Mean sum of errors for Tablet C:\", Tabelt_C[\"sum\"].mean())\n",
    "\n",
    "print(f\"stat={stat}, p={p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statitical analysis Tablet A vs Tablet C\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "Tabelt_A = df[df[\"Tablet\"] == \"A\"]\n",
    "Tabelt_C = df[df[\"Tablet\"] == \"C\"]\n",
    "\n",
    "stat, p = ttest_ind(Tabelt_A[\"sum\"], Tabelt_C[\"sum\"])\n",
    "print(\"Mean sum of errors for Tablet A:\", Tabelt_A[\"sum_abs\"].mean())\n",
    "print(\"Mean sum of errors for Tablet C:\", Tabelt_C[\"sum_abs\"].mean())\n",
    "\n",
    "print(f\"stat={stat}, p={p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a statistically significant differenve between the prediction error on Data from Tablet A and Data from Tablet C. On average Tablet A seems to be more prone to False Positives (Error > 0) and Tablet C more prone to False Negatives (Error < 0). Why the fuck is that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if one Tablet was used at a different time in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate sum of errors with date of admission\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Transform to ordinal\n",
    "df[\"adm_date_ordinal\"] = df[\"adm_date\"].apply(lambda x: x.toordinal())\n",
    "df[\"adm_date_str\"] = df[\"adm_date\"].apply(lambda x: x.strftime(\"%d-%m\"))\n",
    "# Correlate\n",
    "corr, p = pearsonr(df[\"adm_date_ordinal\"], df[\"sum\"])\n",
    "print(f\"Correlation between sum of errors and date of admission: corr={corr}, p={p}\")\n",
    "\n",
    "# Plot adm_date vs sum of errors with tablet as hue\n",
    "sns.set()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"adm_date_str\", y=\"sum\", data=df.sort_values(\"adm_date\"), hue=\"Tablet\", jitter=0.3)\n",
    "# Add a line for the mean\n",
    "sns.lineplot(x=\"adm_date_str\", y=\"sum\", data=df.sort_values(\"adm_date\"), estimator=np.mean)\n",
    "plt.xticks(rotation=90)\n",
    "# Change x label format to only display the day and moth of the admission date\n",
    "plt.xlabel(\"Admission date\")\n",
    "plt.ylabel(\"sum of errors\")\n",
    "plt.title(\"sum of errors for each video\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate\n",
    "corr, p = pearsonr(df[\"adm_date_ordinal\"], df[\"sum_abs\"])\n",
    "print(f\"Correlation between sum_abs of errors and date of admission: corr={corr}, p={p}\")\n",
    "\n",
    "# Plot adm_date vs sum_abs of errors with tablet as hue\n",
    "sns.set()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"adm_date_str\", y=\"sum_abs\", data=df.sort_values(\"adm_date\"), jitter=0.3)\n",
    "sns.lineplot(x=\"adm_date_str\", y=\"sum_abs\", data=df.sort_values(\"adm_date\"), estimator=np.mean)\n",
    "plt.xticks(rotation=90)\n",
    "# Change x label format to only display the day and moth of the admission date\n",
    "plt.xlabel(\"Admission date\")\n",
    "plt.ylabel(\"Sum_abs of errors\")\n",
    "plt.title(\"Sum_abs of errors for each video\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning by months\n",
    "df[\"adm_date_month\"] = df[\"adm_date\"].apply(lambda x: x.strftime(\"%m-%Y\"))\n",
    "df[\"adm_date_month\"] = pd.to_datetime(df[\"adm_date_month\"], format=\"%m-%Y\")\n",
    "# Adm date month as string containint month name\n",
    "df[\"adm_date_month_str\"] = df[\"adm_date_month\"].apply(lambda x: x.strftime(\"%b %Y\"))\n",
    "# df[\"adm_date_month_str\"] = df[\"adm_date_month\"].apply(lambda x: x.strftime(\"%m-%Y\"))\n",
    "\n",
    "# Binning by weeks\n",
    "df[\"adm_date_week\"] = df[\"adm_date\"].apply(lambda x: x.strftime(\"%W-%Y\"))\n",
    "df[\"adm_date_week\"] = df[\"adm_date_week\"].apply(lambda x: int(x[:2]))\n",
    "\n",
    "df[\"adm_date_two_weeks\"] = pd.cut(df[\"adm_date_week\"], bins=8)\n",
    "df[\"adm_date_two_weeks\"] = df[\"adm_date_two_weeks\"].apply(lambda x: str(int(x.left)) + \"-\" + str(int(x.right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate by months\n",
    "corr, p = pearsonr(df[\"adm_date_month\"].apply(lambda x: x.toordinal()), df[\"sum\"])\n",
    "print(f\"Correlation between sum of errors and date of admission (month): corr={corr}, p={p}\")\n",
    "\n",
    "# Correlate by weeks\n",
    "corr, p = pearsonr(df[\"adm_date_week\"], df[\"sum\"])\n",
    "print(f\"Correlation between sum of errors and date of admission (week): corr={corr}, p={p}\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "# Plot adm_date binned monthly vs sum of errors \n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "sns.stripplot(x=\"adm_date_month_str\", y=\"sum\", data=df.sort_values(\"adm_date_month\"))\n",
    "sns.lineplot(x=\"adm_date_month_str\", y=\"sum\", data=df.sort_values(\"adm_date_month\"), estimator=np.mean)\n",
    "# Set y axis range -6 to 6\n",
    "plt.ylim(-6.2, 6.2)\n",
    "# plt.xticks(rotation=90)\n",
    "# Label axes\n",
    "plt.xlabel(\"Month of admission\")\n",
    "plt.ylabel(\"Sum of errors (< 0 = FN, > 0 = FP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlate by months\n",
    "corr, p = pearsonr(df[\"adm_date_month\"].apply(lambda x: x.toordinal()), df[\"sum_abs\"])\n",
    "print(f\"Correlation between sum of errors and date of admission (month): corr={corr}, p={p}\")\n",
    "\n",
    "# Plot adm_date binned monthly vs sum of errors \n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"adm_date_month_str\", y=\"sum_abs\", data=df.sort_values(\"adm_date_month\"))\n",
    "sns.lineplot(x=\"adm_date_month_str\", y=\"sum_abs\", data=df.sort_values(\"adm_date_month\"), estimator=np.mean)\n",
    "# Set y axis range -6 to 6\n",
    "plt.ylim(0, 6.2)\n",
    "plt.xticks(rotation=90)\n",
    "# Label axes\n",
    "plt.xlabel(\"Month of admission\")\n",
    "plt.ylabel(\"Sum of errors (< 0 = FN, > 0 = FP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"adm_date_two_weeks\", y=\"sum\", data=df.sort_values(\"adm_date\"))\n",
    "sns.lineplot(x=\"adm_date_two_weeks\", y=\"sum\", data=df.sort_values(\"adm_date\"), estimator=np.mean)\n",
    "plt.ylim(-6.2, 6.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate sum of errors with manual severity score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "stat, p = pearsonr(df[\"sum\"], df[\"Severity Score\"])\n",
    "print(f\"Severity score correlated with sum of errors: stat={stat:.3f}, p={p:.3f}\")\n",
    "\n",
    "stat, p = pearsonr(df[\"sum_abs\"], df[\"Severity Score\"])\n",
    "print(f\"Severity score correlated with sum_abs of errors: stat={stat:.3f}, p={p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split time into February and March vs April and May and perform t-test\n",
    "df[\"adm_date_first_half\"] = df[\"adm_date_month\"] < pd.to_datetime(\"2021-04-01\")\n",
    "\n",
    "# T-test first half vs second half errors\n",
    "from scipy.stats import ttest_ind\n",
    "feb_march = df[df[\"adm_date_first_half\"] == True][\"sum\"]\n",
    "apr_may = df[df[\"adm_date_first_half\"] == False][\"sum\"]\n",
    "stat, p = ttest_ind(feb_march, apr_may)\n",
    "print(f\"February and March vs April and May: stat={stat:.3f}, p={p:.3f}\")\n",
    "# Print means\n",
    "print(f\"February and March mean: {feb_march.mean():.3f}\")\n",
    "print(f\"April and May mean: {apr_may.mean():.3f}\")\n",
    "\n",
    "# T-test first half vs second half for absolute errors\n",
    "feb_march = df[df[\"adm_date_first_half\"] == True][\"sum_abs\"]\n",
    "apr_may = df[df[\"adm_date_first_half\"] == False][\"sum_abs\"]\n",
    "stat, p = ttest_ind(feb_march, apr_may)\n",
    "print(f\"February and March vs April and May (absolute error): stat={stat:.3f}, p={p:.3f}\")\n",
    "# Print means\n",
    "print(f\"February and March mean: {feb_march.mean():.3f}\")\n",
    "print(f\"April and May mean: {apr_may.mean():.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: Not the absolut error is correlated with the severity score. (higher sev score = higher/lower accuracy). But the directional error (FP vs FN) is correlated with the severity score. (higher sev score = more FP, lower sev score = more FN). The model seems to learn the severity score it just is not strongly related to the covid variable.\n",
    "\n",
    "Error is negatively correlated -> Severity score is more decoupled from the covid diagnosis towards later staged of the pandemic?\n",
    "More FN -> generally lower severity score in the summer/different patient population/ different variant/ more vaccinations? But still people have COVID."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare error type to severity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannot import add_stat_annotation\n",
    "\n",
    "df[\"error_type\"] = df[\"sum\"].apply(lambda x: \"FP\" if x > 0 else \"FN\")\n",
    "# All wrong are the ones that have -6 or 6 as error\n",
    "all_wrong = df[(df[\"sum\"] <= -5) | (df[\"sum\"] >= 5)]\n",
    "# Severity score as integer\n",
    "# all_wrong[\"Severity Score\"] = all_wrong[\"Severity Score\"].astype(int)\n",
    "\n",
    "# Print mean severity score for FP and FN\n",
    "mean_FN = all_wrong[all_wrong[\"error_type\"] == \"FN\"][\"Severity Score\"].mean()\n",
    "mean_FP = all_wrong[all_wrong[\"error_type\"] == \"FP\"][\"Severity Score\"].mean()\n",
    "print(f\"Mean severity score for FN: {mean_FN:.2f} vs FP: {mean_FP:.2f}\")\n",
    "\n",
    "# Plot all wrong as boxplot FN/FP against severity score\n",
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.violinplot(y=\"Severity Score\", x=\"error_type\", data=all_wrong, cut=0)\n",
    "# add_stat_annotation(ax, data=all_wrong, x=\"error_type\", y=\"Severity Score\", box_pairs=[(\"FP\", \"FN\")], test=\"Mann-Whitney\", text_format=\"simple\", loc=\"outside\", verbose=2)\n",
    "# Y ticks as int\n",
    "plt.yticks(np.arange(0, 4, 1))\n",
    "\n",
    "plt.xlabel(\"Error type\")\n",
    "plt.ylabel(\"Manual severity score\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche werden von den Sev scores FN oder FP predicted?  \n",
    "Hypothese: Die immer falsch klassifiziert werden haben eine hohe severity (man lang score) aber halt kein COVID. FP haben hohen sev score. FN haben niedrigen sev score.  \n",
    "--> Classification Models sind am ende eigentlich severity Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = df.groupby(\"Patient ID\").agg({\"sum\": \"sum\", \"sum_abs\":\"sum\",\"COVID19\": \"first\"})\n",
    "patient_df.sort_values(by=\"sum_abs\", ascending=False, inplace=True)\n",
    "patient_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocovid2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b946eb799261426a5ad5f24f4c27986c0f2fe154ec5ad35bd8b05c7d39be50ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
