{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import sys; sys.path.insert(0, \"../utils/\")\n",
    "from data_utils import get_data_location, get_clinical_df, get_bluepoints_df, get_manual_severity_scores\n",
    "from ml_pipeline import ModelEvaluation, highlight_max\n",
    "\n",
    "DATA_PATH = get_data_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load yolo_detections_df if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"yolo_detection_df.csv\"):\n",
    "    yolo_detection_df = pd.read_csv(\"yolo_detection_df.csv\")\n",
    "else:\n",
    "    print(\"Yolo file does not exist\")\n",
    "\n",
    "if os.path.exists(\"df.csv\"):\n",
    "    df = pd.read_csv(\"df.csv\")\n",
    "else:\n",
    "    print(\"df file does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add manual severity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_manual = get_manual_severity_scores()\n",
    "\n",
    "# Merge manual severity scores with yolo severity scores\n",
    "yolo_detection_df = yolo_detection_df.merge(severity_manual[[\"video_name\", \"Severity Score\"]], on=\"video_name\", how=\"left\")\n",
    "\n",
    "# Rename Severity Score to manual_severity_score\n",
    "yolo_detection_df.rename(columns={\"Severity Score\": \"manual_severity_score\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the class names from the yolo net\n",
    "\n",
    "# Class names: ['0: Airbronchograms', '1: Alines', '2: Blines', '3: Bpatch', '4: Consolidations', '5: Pleura', '6: Rib', '7: Shadow']\n",
    "class_names = [\"Airbronchograms\", \"Alines\", \"Blines\", \"Bpatch\", \"Consolidations\", \"Pleura\", \"Rib\", \"Shadow\"]\n",
    "class_name_2_idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "idx_2_class_name = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "# yolo_detection_df[\"class_name\"] = yolo_detection_df[\"class\"].apply(lambda x: class_names[x])\n",
    "\n",
    "idx_2_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataframe contains one row per object detected by the YOLONet in all frames of all bluepoint-videos of all patients\n",
    "# The columns are:\n",
    "# - class: the class of the object detected by the YOLONet\n",
    "# - confidence: the confidence of the YOLONet in the detection\n",
    "# - x: the x-coordinate of the center of the bounding box\n",
    "# - y: the y-coordinate of the center of the bounding box\n",
    "# - w: the width of the bounding box\n",
    "# - h: the height of the bounding box\n",
    "# - area: the area of the bounding box\n",
    "# - video_name: the name of the video\n",
    "# - Frame: the frame number\n",
    "#...\n",
    "# - Patient ID: the ID of the patient\n",
    "# - Bluepoint: the name of the bluepoint\n",
    "# - COVID19: the clinical diagnosis of the patient\n",
    "# - yolo_quality_score: the quality score of the YOLONet\n",
    "# - yolo_quality: the quality of the YOLONet\n",
    "# - yolo_severity_score: the severity score of the YOLONet\n",
    "# - class_name: the name of the class\n",
    "yolo_detection_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_detection_df[\"yolo_quality_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibly filter the df before analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out detections with low quality\n",
    "# yolo_detection_df = yolo_detection_df[yolo_detection_df[\"yolo_quality\"] == \"Excellent,\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity Score analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame-wise severity score\n",
    "frame_level = yolo_detection_df.groupby([\"Frame\", \"video_name\"]).first().reset_index().drop(columns=[\"class\", \"confidence\", \"x\", \"y\", \"w\", \"h\", \"area\", \"class_name\"])\n",
    "\n",
    "# Filter out frames with low quality\n",
    "severity_filtered = frame_level[frame_level.yolo_severity_score >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_detection_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class area analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding information whether class is pathological or not. For now I assume class 0, 2, 3 and 4 are pathological\n",
    "# Class names: ['0: Airbronchograms', '1: Alines', '2: Blines', '3: Bpatch', '4: Consolidations', '5: Pleura', '6: Rib', '7: Shadow']\n",
    "pathological_classes = [0, 2, 3, 4]\n",
    "non_pathological_classes = [1, 5, 6, 7]\n",
    "pathological_class_names = [class_names[x] for x in pathological_classes]\n",
    "non_pathological_class_names = [class_names[x] for x in non_pathological_classes]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate manual severity score with class count of different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_detection_df_pivot = yolo_detection_df.pivot_table(index=[\"video_name\", \"Frame\"], columns=\"class_name\", values=\"area\", aggfunc=\"count\").fillna(0).reset_index()\n",
    "# Add Bluepoint, Patient ID and manual severity score\n",
    "yolo_detection_df_pivot = yolo_detection_df_pivot.merge(yolo_detection_df[[\"video_name\", \"Frame\", \"Bluepoint\", \"Patient ID\", \"manual_severity_score\", \"COVID19\"]], on=[\"video_name\", \"Frame\"], how=\"left\")\n",
    "yolo_detection_df_pivot = yolo_detection_df_pivot.drop_duplicates().sort_values(by=[\"Patient ID\", \"Bluepoint\", \"Frame\"]).reset_index(drop=True)\n",
    "yolo_detection_df_pivot.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frame level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Plot and correlate manual severity scores with class areas\n",
    "# Boxplot for each test_column\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    cl = class_names[i]\n",
    "    sns.boxplot(data=yolo_detection_df_pivot, y=cl, x=\"manual_severity_score\", ax=ax)\n",
    "    corr = yolo_detection_df_pivot.manual_severity_score.corr(yolo_detection_df_pivot[cl])\n",
    "    ax.set_title(f\"{cl} (r = {corr:.2f})\")\n",
    "\n",
    "plt.suptitle(\"Frame level correlation of manual severity scores with class areas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_detection_df_pivot[\"pathological_class_count\"] = yolo_detection_df_pivot[pathological_class_names].sum(axis=1)\n",
    "yolo_detection_df_pivot[\"non_pathological_class_count\"] = yolo_detection_df_pivot[non_pathological_class_names].sum(axis=1)\n",
    "yolo_detection_df_pivot\n",
    "\n",
    "# Correlate manual severity scores with pathological vs non_pathological class counts\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "path_list = [\"pathological_class_count\", \"non_pathological_class_count\"]\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    path = path_list[i]\n",
    "    sns.boxplot(data=yolo_detection_df_pivot, y=path, x=\"manual_severity_score\", ax=ax)\n",
    "    corr = yolo_detection_df_pivot.manual_severity_score.corr(yolo_detection_df_pivot[path])\n",
    "    ax.set_title(f\"{path} (r = {corr:.2f})\")\n",
    "\n",
    "plt.suptitle(\"Frame level correlation of manual severity scores with pathological vs non-pathological class counts\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by video and sum all pixels\n",
    "grp = yolo_detection_df_pivot.groupby([\"video_name\"])\n",
    "agg_dict = {cl: \"mean\" for cl in class_names}\n",
    "agg_dict[\"manual_severity_score\"] = \"first\"\n",
    "video_lvl_mean = grp.agg(agg_dict).reset_index()\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Plot and correlate manual severity scores with class areas\n",
    "# Boxplot for each test_column\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    cl = class_names[i]\n",
    "    sns.boxplot(data=video_lvl_mean, y=cl, x=\"manual_severity_score\", ax=ax)\n",
    "    # Add regression line to plot\n",
    "    sns.regplot(data=video_lvl_mean, y=cl, x=\"manual_severity_score\", ax=ax, scatter=False)\n",
    "    corr = video_lvl_mean.manual_severity_score.corr(video_lvl_mean[cl])\n",
    "    ax.set_title(f\"{cl} (r = {corr:.2f})\")\n",
    "\n",
    "plt.suptitle(\"Video level correlation of manual severity scores with class areas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_lvl_mean[\"pathological_class_count\"] = video_lvl_mean[pathological_class_names].sum(axis=1)\n",
    "video_lvl_mean[\"non_pathological_class_count\"] = video_lvl_mean[non_pathological_class_names].sum(axis=1)\n",
    "\n",
    "# Correlate manual severity scores with pathological vs non_pathological class counts\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "path_list = [\"pathological_class_count\", \"non_pathological_class_count\"]\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    path = path_list[i]\n",
    "    sns.boxplot(data=video_lvl_mean, y=path, x=\"manual_severity_score\", ax=ax)\n",
    "    # Add regression line to plot\n",
    "    sns.regplot(data=video_lvl_mean, y=path, x=\"manual_severity_score\", ax=ax, scatter=False)\n",
    "    corr = video_lvl_mean.manual_severity_score.corr(video_lvl_mean[path])\n",
    "    ax.set_title(f\"{path} (r = {corr:.2f})\")\n",
    "\n",
    "plt.suptitle(\"Frame level correlation of manual severity scores with pathological vs non-pathological class counts\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patient Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by video and sum all pixels\n",
    "grp = yolo_detection_df_pivot.groupby([\"Patient ID\"])\n",
    "agg_dict = {cl: \"mean\" for cl in class_names}\n",
    "agg_dict[\"manual_severity_score\"] = \"median\"\n",
    "patient_lvl_mean = grp.agg(agg_dict).reset_index()\n",
    "# manual_severity_score as int\n",
    "patient_lvl_mean[\"manual_severity_score\"] = patient_lvl_mean[\"manual_severity_score\"].astype(int)\n",
    "\n",
    "sns.set(font_scale=1.7)\n",
    "\n",
    "# Plot and correlate manual severity scores with class areas\n",
    "# Boxplot for each test_column\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    cl = class_names[i]\n",
    "    sns.boxplot(data=patient_lvl_mean, y=cl, x=\"manual_severity_score\", ax=ax).set_xlabel(\"Manual severity score\")\n",
    "    # Add regression line to plot\n",
    "    sns.regplot(data=patient_lvl_mean, y=cl, x=\"manual_severity_score\", ax=ax, scatter=False)\n",
    "\n",
    "    corr = patient_lvl_mean.manual_severity_score.corr(patient_lvl_mean[cl])\n",
    "    ax.set_title(f\"{cl} (r = {corr:.2f})\")\n",
    "    if i == 0 or i == 4:\n",
    "        ax.set_ylabel(\"Avrg. Class Count / Frame\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"\")\n",
    "    if i > 3:\n",
    "        ax.set_xlabel(\"Manual Severity Score\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "    #ax.set_xlabel(\"Manual severity score\")\n",
    "    #ax.set_ylabel(\"Avrg. Class Count / Frame\")\n",
    "\n",
    "plt.suptitle(\"Patient-level Correlation of Median Manual Severity Scores with Class Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_lvl_mean[\"pathological_class_count\"] = patient_lvl_mean[pathological_class_names].sum(axis=1)\n",
    "patient_lvl_mean[\"non_pathological_class_count\"] = patient_lvl_mean[non_pathological_class_names].sum(axis=1)\n",
    "\n",
    "# Correlate manual severity scores with pathological vs non_pathological class counts\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "path_list = [\"pathological_class_count\", \"non_pathological_class_count\"]\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    path = path_list[i]\n",
    "    sns.boxplot(data=patient_lvl_mean, y=path, x=\"manual_severity_score\", ax=ax)\n",
    "    # Add regression line to plot\n",
    "    sns.regplot(data=patient_lvl_mean, y=path, x=\"manual_severity_score\", ax=ax, scatter=False)\n",
    "    corr = patient_lvl_mean.manual_severity_score.corr(patient_lvl_mean[path])\n",
    "    ax.set_title(f\"{path} (r = {corr:.2f})\")\n",
    "\n",
    "plt.suptitle(\"Frame level correlation of manual severity scores with pathological vs non-pathological class counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add class count, weightes class count and weighted class area by confidence\n",
    "yolo_detection_df[\"area_weighted\"] = yolo_detection_df[\"area\"] * yolo_detection_df[\"confidence\"]\n",
    "grp = yolo_detection_df.groupby([\"video_name\", \"class\"])\n",
    "video_lvl = grp.agg({\"class\": \"count\", \"confidence\": \"sum\", \"area\": \"sum\", \"area_weighted\": \"sum\"})\n",
    "video_lvl = video_lvl.rename(columns={\"class\": \"class_count\", \"confidence\": \"class_count_conf_weighted\", \"area\": \"area\", \"area_weighted\": \"area_conf_weighted\"}).reset_index()\n",
    "\n",
    "# Add frame count\n",
    "frame_count_per_video = yolo_detection_df[[\"video_name\", \"Frame\"]].drop_duplicates().groupby([\"video_name\"]).count().reset_index().rename(columns={\"Frame\": \"Frame_count\"})\n",
    "video_lvl = pd.merge(video_lvl, frame_count_per_video, on=\"video_name\")\n",
    "\n",
    "\n",
    "# Devide by frame count to account for different video lengths\n",
    "video_lvl[\"area\"] = video_lvl[\"area\"] / video_lvl[\"Frame_count\"]\n",
    "video_lvl[\"area_conf_weighted\"] = video_lvl[\"area_conf_weighted\"] / video_lvl[\"Frame_count\"]\n",
    "video_lvl[\"class_count\"] = video_lvl[\"class_count\"] / video_lvl[\"Frame_count\"]\n",
    "video_lvl[\"class_count_conf_weighted\"] = video_lvl[\"class_count_conf_weighted\"] / video_lvl[\"Frame_count\"]\n",
    "\n",
    "# Add classes with 0 area\n",
    "possible_classes = list(range(len(class_names)))\n",
    "video_names = yolo_detection_df.video_name.unique().tolist()\n",
    "from itertools import product\n",
    "idx_df =  pd.DataFrame(list(product(possible_classes,video_names))).rename(columns={0:'class',1:'video_name'})\n",
    "\n",
    "video_lvl = pd.merge(idx_df, video_lvl, on=[\"video_name\", \"class\"], how=\"left\").fillna(0).sort_values(by=[\"video_name\", \"class\"])\n",
    "\n",
    "video_lvl[\"class_name\"] = video_lvl[\"class\"].map(lambda x: class_names[x])\n",
    "\n",
    "patient_data = yolo_detection_df.groupby([\"Patient ID\", \"video_name\"]).agg({\"COVID19\": \"first\",\"Bluepoint\": \"first\", \"yolo_severity_score\": \"mean\", \"manual_severity_score\": \"first\"}).reset_index()\n",
    "video_level = pd.merge(video_lvl, patient_data, on=\"video_name\").sort_values(by=[\"Patient ID\", \"Bluepoint\", \"class\"]).reset_index(drop=True)\n",
    "# Reorder columns\n",
    "video_level = video_level[[\"Patient ID\", \"COVID19\", \"Bluepoint\", \"video_name\", \"yolo_severity_score\", \"class\", \"class_name\", \"class_count\", \"class_count_conf_weighted\", \"area\", \"area_conf_weighted\", \"manual_severity_score\"]]\n",
    "video_level.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To beginn we try 4 different methods to aggregate the classes over time. \n",
    "1. Simple class count per video divided by the number of frames (avrg number of classes per frame).\n",
    "2. The class count weighted by the networks confidence divided by n frames.\n",
    "3. The area of the detected classes summed up divided by n frames. Testing the hypothesis that the affected area has a predictive power for covid.\n",
    "4. The area weighted by the networks confidence /n_Frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different standard models using all class areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ml_pipeline import ModelEvaluation\n",
    "\n",
    "Regressor = ModelEvaluation(mode=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models on accumulated area of all detected classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \"area_conf_weighted\", \"class_count\", \"class_count_conf_weighted\"]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_video_pivot = video_level.pivot(index=\"video_name\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_video_pivot = pd.merge(class_area_per_video_pivot, video_level[[\"video_name\", \"COVID19\", \"Bluepoint\", \"Patient ID\", \"manual_severity_score\"]], on=\"video_name\")\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.dropna(subset=[\"manual_severity_score\"])\n",
    "    # Shuffle\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Split into X and y\n",
    "    X = class_area_per_video_pivot[class_names]\n",
    "    y = class_area_per_video_pivot[\"manual_severity_score\"]\n",
    "    groups = class_area_per_video_pivot[\"Patient ID\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, groups=groups, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models performance of 57% accuracy is not great but comparable to our first approaches of training the NN. It can't compete with the best values of the severity analysis so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statannot import add_stat_annotation\n",
    "from itertools import product\n",
    "\n",
    "pairs = [('area', 'class_count'),\n",
    "        ('area_conf_weighted', 'area'),\n",
    "        ('class_count_conf_weighted', 'class_count')]\n",
    "\n",
    "scores_all_classes = agg_scores.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(data=scores_all_classes, x=\"Aggregation method\", y=\"test_r2\", palette=\"Set3\")\n",
    "add_stat_annotation(ax, data=scores_all_classes, x=\"Aggregation method\", y=\"test_r2\",\n",
    "                    box_pairs=pairs,\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "plt.title(\"Accuracy of different aggregation methods for all classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the different aggregation strategies show that incorporating the networks confidence does not significantly improve the network.\n",
    "We can also see that the class count is not significantly more predicitve than the summed up area. This suggests to reject the hyptothesis that the area for this network has a significant informative value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if restricting to pathological classes makes a change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding information whether class is pathological or not. For now I assume class 0, 2, 3 and 4 are pathological\n",
    "# Class names: ['0: Airbronchograms', '1: Alines', '2: Blines', '3: Bpatch', '4: Consolidations', '5: Pleura', '6: Rib', '7: Shadow']\n",
    "pathological_classes = [0, 2, 3, 4]\n",
    "non_pathological_classes = [1, 5, 6, 7]\n",
    "pathological_class_names = [class_names[x] for x in pathological_classes]\n",
    "non_pathological_class_names = [class_names[x] for x in non_pathological_classes]\n",
    "video_level[\"pathological_class\"] = video_level[\"class\"].apply(lambda x: 1 if x in pathological_classes else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models on aggregated class (area) of all pathological classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \"area_conf_weighted\", \"class_count\", \"class_count_conf_weighted\"]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_video_pivot = video_level.pivot(index=\"video_name\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_video_pivot = pd.merge(class_area_per_video_pivot, video_level[[\"video_name\", \"COVID19\", \"Bluepoint\", \"Patient ID\", \"manual_severity_score\"]], on=\"video_name\")\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.dropna(subset=[\"manual_severity_score\"])\n",
    "    # Shuffle\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Split into X and y\n",
    "    X = class_area_per_video_pivot[pathological_class_names]\n",
    "    y = class_area_per_video_pivot[\"manual_severity_score\"]\n",
    "    groups = class_area_per_video_pivot[\"Patient ID\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, groups=groups, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statannot import add_stat_annotation\n",
    "from itertools import product\n",
    "\n",
    "pairs = [('area', 'class_count'),\n",
    "        ('area_conf_weighted', 'area'),\n",
    "        ('class_count_conf_weighted', 'class_count'),\n",
    "        ]\n",
    "\n",
    "scores_pathological_classes = agg_scores.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.boxplot(data=scores_pathological_classes, x=\"Aggregation method\", y=\"test_r2\", palette=\"Set3\")\n",
    "add_stat_annotation(ax, data=scores_pathological_classes, x=\"Aggregation method\", y=\"test_r2\",\n",
    "                    box_pairs=pairs,\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=0)\n",
    "plt.title(\"Accuracy of different aggregation methods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When restricting to the pathological classes there is no significant difference between either area or class_count and no significant difference when weighting by the networks confidence.\n",
    "\n",
    "The mean of the class count is the highest which is why we will proceed using the class count as primary aggregation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U-Test\n",
    "x0 = scores_all_classes[\"test_r2\"]\n",
    "x1 = scores_pathological_classes[\"test_r2\"]\n",
    "stat, p_value = mannwhitneyu(x0, x1)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean r2 all classes: {x0.mean():.4f}\")\n",
    "print(f\"Mean r2 pathological classes: {x1.mean():.4f}\")\n",
    "\n",
    "# Print green if p-value is significant otherwise red\n",
    "if p_value < 0.05:\n",
    "    print(\"\\033[92mSignificant difference\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91mNo significant difference\\033[0m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on all trained models and all used aggregation strategies shows that restricting the model input to the pathological classes is not significantly better $(p > 0.05)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to some extend not suprising since the information about the pathological classes is also contained in the full set of class names. Instead we check if restricting the model to only non-pathologocal classes decreases performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \"area_conf_weighted\", \"class_count\", \"class_count_conf_weighted\"]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_video_pivot = video_level.pivot(index=\"video_name\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_video_pivot = pd.merge(class_area_per_video_pivot, video_level[[\"video_name\", \"COVID19\", \"Bluepoint\", \"Patient ID\", \"manual_severity_score\"]], on=\"video_name\")\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.dropna(subset=[\"manual_severity_score\"])\n",
    "    # Shuffle\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Split into X and y\n",
    "    X = class_area_per_video_pivot[non_pathological_class_names]\n",
    "    y = class_area_per_video_pivot[\"manual_severity_score\"]\n",
    "    groups = class_area_per_video_pivot[\"Patient ID\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, groups=groups, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statannot import add_stat_annotation\n",
    "from itertools import product\n",
    "\n",
    "pairs = [('area', 'class_count'),\n",
    "        ('area_conf_weighted', 'area'),\n",
    "        ('class_count_conf_weighted', 'class_count'),\n",
    "        ]\n",
    "\n",
    "scores_non_pathological_classes = agg_scores.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.boxplot(data=scores_non_pathological_classes, x=\"Aggregation method\", y=\"test_r2\", palette=\"Set3\")\n",
    "add_stat_annotation(ax, data=scores_non_pathological_classes, x=\"Aggregation method\", y=\"test_r2\",\n",
    "                    box_pairs=pairs,\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=0)\n",
    "plt.title(\"Accuracy of different aggregation methods\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U-Test\n",
    "x0 = scores_all_classes[\"test_r2\"]\n",
    "x1 = scores_non_pathological_classes[\"test_r2\"]\n",
    "stat, p_value = mannwhitneyu(x0, x1)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean test_r2 all classes: {x0.mean():.4f}\")\n",
    "print(f\"Mean test_r2 non pathological classes: {x1.mean():.4f}\")\n",
    "\n",
    "# Print green if p-value is significant otherwise red\n",
    "if p_value < 0.05:\n",
    "    print(\"\\033[92mSignificant difference\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91mNo significant difference\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all classes vs using only non-pathological classes for training makes a significant difference across all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient level area analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add class count, weightes class count and weighted class area by confidence\n",
    "yolo_detection_df[\"area_weighted\"] = yolo_detection_df[\"area\"] * yolo_detection_df[\"confidence\"]\n",
    "grp = yolo_detection_df.groupby([\"Patient ID\", \"class\"])\n",
    "patient_lvl = grp.agg({\"class\": \"count\", \"confidence\": \"sum\", \"area\": \"sum\", \"area_weighted\": \"sum\"})\n",
    "patient_lvl = patient_lvl.rename(columns={\"class\": \"class_count\", \"confidence\": \"class_count_conf_weighted\", \"area\": \"area\", \"area_weighted\": \"area_conf_weighted\"}).reset_index()\n",
    "\n",
    "# Add frame count\n",
    "frame_count_per_patient = yolo_detection_df[[\"Patient ID\", \"Bluepoint\", \"Frame\"]].drop_duplicates().groupby([\"Patient ID\"]).count().reset_index().rename(columns={\"Frame\": \"Frame_count\"}).drop(columns=[\"Bluepoint\"])\n",
    "patient_lvl = pd.merge(patient_lvl, frame_count_per_patient, on=\"Patient ID\")\n",
    "\n",
    "\n",
    "# Devide by frame count to get the average area per frame\n",
    "patient_lvl[\"area\"] = patient_lvl[\"area\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"area_conf_weighted\"] = patient_lvl[\"area_conf_weighted\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"class_count\"] = patient_lvl[\"class_count\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"class_count_conf_weighted\"] = patient_lvl[\"class_count_conf_weighted\"] / patient_lvl[\"Frame_count\"]\n",
    "\n",
    "\n",
    "# Add classes with 0 area\n",
    "possible_classes = list(range(len(class_names)))\n",
    "patient_names = yolo_detection_df[\"Patient ID\"].unique().tolist()\n",
    "from itertools import product\n",
    "idx_df =  pd.DataFrame(list(product(possible_classes, patient_names))).rename(columns={0:'class',1:'Patient ID'})\n",
    "\n",
    "patient_lvl = pd.merge(idx_df, patient_lvl, on=[\"Patient ID\", \"class\"], how=\"left\").fillna(0).sort_values(by=[\"Patient ID\", \"class\"])\n",
    "\n",
    "patient_lvl[\"class_name\"] = patient_lvl[\"class\"].map(lambda x: class_names[x])\n",
    "\n",
    "patient_data = yolo_detection_df.groupby([\"Patient ID\"]).agg({\"COVID19\": \"first\", \"yolo_severity_score\": \"mean\", \"manual_severity_score\": \"mean\"}).reset_index()\n",
    "patient_lvl = pd.merge(patient_lvl, patient_data, on=\"Patient ID\").sort_values(by=[\"Patient ID\", \"class\"]).reset_index(drop=True)\n",
    "# Reorder columns\n",
    "patient_lvl = patient_lvl[[\"Patient ID\", \"COVID19\", \"yolo_severity_score\", \"class\", \"class_name\", \"class_count\", \"class_count_conf_weighted\", \"area\", \"area_conf_weighted\", \"manual_severity_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \n",
    "            \"area_conf_weighted\", \n",
    "            \"class_count\", \n",
    "            \"class_count_conf_weighted\"\n",
    "            ]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_patient_pivot = patient_lvl.pivot(index=\"Patient ID\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_patient_pivot = pd.merge(class_area_per_patient_pivot, patient_lvl[[\"Patient ID\" ,\"COVID19\", \"manual_severity_score\"]], on=\"Patient ID\")\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    # Shuffle\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Use all classes for the model\n",
    "    X = class_area_per_patient_pivot[class_names]\n",
    "    y = class_area_per_patient_pivot[\"manual_severity_score\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('area', 'class_count'),\n",
    "        # ('area_conf_weighted', 'area'),\n",
    "        # ('class_count_conf_weighted', 'class_count'),\n",
    "        ]\n",
    "\n",
    "scores_all_classes = agg_scores.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=scores_all_classes, x=\"Aggregation method\", y=\"test_r2\", palette=\"Set3\")\n",
    "add_stat_annotation(ax, data=scores_all_classes, x=\"Aggregation method\", y=\"test_r2\",\n",
    "                    box_pairs=pairs,\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "plt.title(\"Accuracy of different aggregation methods for all classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_mean = scores_all_classes[scores_all_classes[\"Aggregation method\"].isin([\"area\"])].mean().loc[\"test_r2\"]\n",
    "area_std = scores_all_classes[scores_all_classes[\"Aggregation method\"].isin([\"area\"])].std().loc[\"test_r2\"]\n",
    "class_count_mean = scores_all_classes[scores_all_classes[\"Aggregation method\"].isin([\"class_count\"])].mean().loc[\"test_r2\"]\n",
    "class_count_std = scores_all_classes[scores_all_classes[\"Aggregation method\"].isin([\"class_count\"])].std().loc[\"test_r2\"]\n",
    "\n",
    "print(f\"Area mean: {area_mean:.2f} +- {area_std:.2f}\")\n",
    "print(f\"Class count mean: {class_count_mean:.2f} +- {class_count_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the patient level improves the results significantly. Similarly to before try using only pathological classes for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \n",
    "            \"area_conf_weighted\", \n",
    "            \"class_count\", \n",
    "            \"class_count_conf_weighted\"\n",
    "            ]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_patient_pivot = patient_lvl.pivot(index=\"Patient ID\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_patient_pivot = pd.merge(class_area_per_patient_pivot, patient_lvl[[\"Patient ID\" ,\"COVID19\", \"manual_severity_score\"]], on=\"Patient ID\")\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    # Shuffle\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Use all classes for the model\n",
    "    X = class_area_per_patient_pivot[pathological_class_names]\n",
    "    y = class_area_per_patient_pivot[\"manual_severity_score\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing the results the filtering for only the pathological classes makes for a slight improvement on the patient level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pathological_classes = agg_scores.reset_index()\n",
    "# Mann-Whitney U-Test\n",
    "x0 = scores_all_classes[\"test_r2\"]\n",
    "x1 = scores_pathological_classes[\"test_r2\"]\n",
    "stat, p_value = mannwhitneyu(x0, x1)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean accuracy all classes: {x0.mean():.4f}\")\n",
    "print(f\"Mean accuracy pathological classes: {x1.mean():.4f}\")\n",
    "\n",
    "# Print green if p-value is significant otherwise red\n",
    "if p_value < 0.05:\n",
    "    print(\"\\033[92mSignificant difference\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91mNo significant difference\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \n",
    "            \"area_conf_weighted\", \n",
    "            \"class_count\", \n",
    "            \"class_count_conf_weighted\"\n",
    "            ]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_patient_pivot = patient_lvl.pivot(index=\"Patient ID\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_patient_pivot = pd.merge(class_area_per_patient_pivot, patient_lvl[[\"Patient ID\" ,\"COVID19\", \"manual_severity_score\"]], on=\"Patient ID\")\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    # Shuffle\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Use all classes for the model\n",
    "    X = class_area_per_patient_pivot[non_pathological_class_names]\n",
    "    y = class_area_per_patient_pivot[\"manual_severity_score\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_non_pathological_classes = agg_scores.reset_index()\n",
    "# Mann-Whitney U-Test\n",
    "x0 = scores_all_classes[\"test_r2\"]\n",
    "x1 = scores_non_pathological_classes[\"test_r2\"]\n",
    "stat, p_value = mannwhitneyu(x0, x1)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean accuracy all classes: {x0.mean():.4f}\")\n",
    "print(f\"Mean accuracy non-pathological classes: {x1.mean():.4f}\")\n",
    "\n",
    "# Print green if p-value is significant otherwise red\n",
    "if p_value < 0.05:\n",
    "    print(\"\\033[92mSignificant difference\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91mNo significant difference\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluepoint information added to class area on video level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \n",
    "            \"area_conf_weighted\", \n",
    "            \"class_count\", \n",
    "            \"class_count_conf_weighted\"\n",
    "            ]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_patient_pivot = patient_lvl.pivot(index=\"Patient ID\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_patient_pivot = pd.merge(class_area_per_patient_pivot, patient_lvl[[\"Patient ID\" ,\"COVID19\", \"manual_severity_score\"]], on=\"Patient ID\")\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    # Shuffle\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Filter out video with Bluepoint == None\n",
    "    class_area_per_video_pivot = class_area_per_video_pivot[class_area_per_video_pivot[\"Bluepoint\"] != 'None']\n",
    "    # Get list of Bluepoint values\n",
    "    bluepoints = class_area_per_video_pivot.Bluepoint.unique().tolist()\n",
    "    bluepoints.sort()\n",
    "    # Create a dictionary for mapping Bluepoint values to integers\n",
    "    bp2code = {bp: i for i, bp in enumerate(bluepoints)}\n",
    "    code2bp = {i: bp for i, bp in enumerate(bluepoints)}\n",
    "    # Map Bluepoint values to integers \n",
    "    class_area_per_video_pivot[\"Bluepoint_codes\"] = class_area_per_video_pivot[\"Bluepoint\"].apply(lambda x: bp2code[x])\n",
    "\n",
    "    X = class_area_per_video_pivot[class_names + [\"Bluepoint_codes\"]]\n",
    "    y = class_area_per_video_pivot[\"manual_severity_score\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, plot=False)\n",
    "\n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_all_classes_plus_pb = agg_scores.reset_index()\n",
    "# Mann-Whitney U-Test\n",
    "x0 = scores_all_classes[\"test_r2\"]\n",
    "x1 = scores_all_classes_plus_pb[\"test_r2\"]\n",
    "stat, p_value = mannwhitneyu(x0, x1)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean accuracy all classes: {x0.mean():.4f}\")\n",
    "print(f\"Mean accuracy classes + bp: {x1.mean():.4f}\")\n",
    "\n",
    "# Print green if p-value is significant otherwise red\n",
    "if p_value < 0.05:\n",
    "    print(\"\\033[92mSignificant difference\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91mNo significant difference\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding information about where the video was taken interestingly does not improve the performance compared to the model simply using all pathological classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training one model per Bluepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluepoint_models = pd.DataFrame()\n",
    "bluepoint_models_all = pd.DataFrame()\n",
    "for bp in bluepoints:\n",
    "    print(f\"Training model for {bp}...\")\n",
    "    bp_df = class_area_per_video_pivot[class_area_per_video_pivot[\"Bluepoint\"] == bp]\n",
    "    X = bp_df[class_names]\n",
    "    y = bp_df[\"manual_severity_score\"]\n",
    "\n",
    "    scores, scores_mean = Regressor.train_models(X, y, plot=False)\n",
    "    scores = pd.concat([scores], keys=[bp], names=['Bluepoint'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[bp], names=['Bluepoint'])\n",
    "\n",
    "    bluepoint_models_all = pd.concat([bluepoint_models_all, scores])\n",
    "    bluepoint_models = pd.concat([bluepoint_models, scores_mean])\n",
    "\n",
    "\n",
    "bluepoint_models.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluepoint_models = bluepoint_models.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(data=bluepoint_models, x=\"Bluepoint\", y=\"test_r2\", palette=\"Set3\")\n",
    "plt.title(\"Accuracy of different bluepoint model ensembles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning L1 and R1 to the bilateral bluepoint \"1\", L2 and R2 to \"2\" and L3 and R3 to \"3\" for aggregated plotting\n",
    "bluepoint_models[\"Bluepoint_bilateral\"] = bluepoint_models[\"Bluepoint\"].apply(lambda x: x[1])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(data=bluepoint_models, x=\"Bluepoint_bilateral\", y=\"test_r2\", palette=\"Set3\")\n",
    "plt.title(\"Accuracy of different bluepoint model ensembles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_left = bluepoint_models_all.loc[[\"L1\", \"L2\", \"L3\"]].test_r2\n",
    "bp_right = bluepoint_models_all.loc[[\"R1\", \"R2\", \"R3\"]].test_r2\n",
    "\n",
    "bp_1 = bluepoint_models_all.loc[[\"L1\", \"R1\"]].test_r2\n",
    "bp_2 = bluepoint_models_all.loc[[\"L2\", \"R2\"]].test_r2\n",
    "bp_3 = bluepoint_models_all.loc[[\"L3\", \"R3\"]].test_r2\n",
    "\n",
    "p_values = []\n",
    "\n",
    "# Mann-Whitney U-Test\n",
    "stat, p_value = mannwhitneyu(bp_left, bp_right)\n",
    "p_values.append(p_value)\n",
    "print(\"Bluepoint left vs right\")\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(f\"Mean test_r2 left: {bp_left.mean():.4f}\")\n",
    "print(f\"Mean test_r2 right: {bp_right.mean():.4f}\")\n",
    "\n",
    "print(\"Bluepoint 1 vs 2 vs 3\")\n",
    "print(\"1 vs 2\")\n",
    "stat, p_value = mannwhitneyu(bp_1, bp_2)\n",
    "p_values.append(p_value)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(\"2 vs 3\")\n",
    "stat, p_value = mannwhitneyu(bp_2, bp_3)\n",
    "p_values.append(p_value)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "print(\"1 vs 3\")\n",
    "stat, p_value = mannwhitneyu(bp_1, bp_3)\n",
    "p_values.append(p_value)\n",
    "print(f\"Mann–Whitney U Test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n",
    "\n",
    "print(\"\\n Means:\")\n",
    "print(f\"Mean test_r2 1: {bp_1.mean():.4f}\")\n",
    "print(f\"Mean test_r2 2: {bp_2.mean():.4f}\")\n",
    "print(f\"Mean test_r2 3: {bp_3.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bonferonni correction for multiple comparisons\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "p_adjusted = multipletests(p_values, method='bonferroni')\n",
    "\n",
    "print(\"Bonferroni corrected p-values:\")\n",
    "\n",
    "test_distributions = [\"L vs R\", \"1 vs 2\", \"2 vs 3\", \"1 vs 3\"]\n",
    "# Print green if p-value is significant otherwise red\n",
    "for i, p in enumerate(p_adjusted[1]):\n",
    "    if p < 0.05:\n",
    "        print(f\"\\033[92m{test_distributions[i]}: {p:.4f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[91m{test_distributions[i]}: {p:.4f}\\033[0m\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models trained on L1 and R1 perform significantly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train only on BP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out video with Bluepoint not L1 or R1\n",
    "yolo_detection_df_filtered = yolo_detection_df[yolo_detection_df[\"Bluepoint\"].isin([\"L1\", \"R1\"])]\n",
    "\n",
    "grp = yolo_detection_df_filtered.groupby([\"Patient ID\", \"class\"])\n",
    "patient_lvl = grp.agg({\"class\": \"count\", \"confidence\": \"sum\", \"area\": \"sum\", \"area_weighted\": \"sum\"})\n",
    "patient_lvl = patient_lvl.rename(columns={\"class\": \"class_count\", \"confidence\": \"class_count_conf_weighted\", \"area\": \"area\", \"area_weighted\": \"area_conf_weighted\"}).reset_index()\n",
    "\n",
    "# Add frame count\n",
    "frame_count_per_patient = yolo_detection_df_filtered[[\"Patient ID\", \"Bluepoint\", \"Frame\"]].drop_duplicates().groupby([\"Patient ID\"]).count().reset_index().rename(columns={\"Frame\": \"Frame_count\"}).drop(columns=[\"Bluepoint\"])\n",
    "patient_lvl = pd.merge(patient_lvl, frame_count_per_patient, on=\"Patient ID\")\n",
    "\n",
    "\n",
    "# Devide by frame count to get the average area per frame\n",
    "patient_lvl[\"area\"] = patient_lvl[\"area\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"area_conf_weighted\"] = patient_lvl[\"area_conf_weighted\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"class_count\"] = patient_lvl[\"class_count\"] / patient_lvl[\"Frame_count\"]\n",
    "patient_lvl[\"class_count_conf_weighted\"] = patient_lvl[\"class_count_conf_weighted\"] / patient_lvl[\"Frame_count\"]\n",
    "\n",
    "\n",
    "# Add classes with 0 area\n",
    "possible_classes = list(range(len(class_names)))\n",
    "patient_names = yolo_detection_df_filtered[\"Patient ID\"].unique().tolist()\n",
    "from itertools import product\n",
    "idx_df =  pd.DataFrame(list(product(possible_classes, patient_names))).rename(columns={0:'class',1:'Patient ID'})\n",
    "\n",
    "patient_lvl = pd.merge(idx_df, patient_lvl, on=[\"Patient ID\", \"class\"], how=\"left\").fillna(0).sort_values(by=[\"Patient ID\", \"class\"])\n",
    "\n",
    "patient_lvl[\"class_name\"] = patient_lvl[\"class\"].map(lambda x: class_names[x])\n",
    "\n",
    "patient_data = yolo_detection_df_filtered.groupby([\"Patient ID\"]).agg({\"COVID19\": \"first\", \"yolo_severity_score\": \"mean\", \"manual_severity_score\": \"mean\"}).reset_index()\n",
    "patient_lvl = pd.merge(patient_lvl, patient_data, on=\"Patient ID\").sort_values(by=[\"Patient ID\", \"class\"]).reset_index(drop=True)\n",
    "# Reorder columns\n",
    "patient_lvl = patient_lvl[[\"Patient ID\", \"COVID19\", \"yolo_severity_score\", \"class\", \"class_name\", \"class_count\", \"class_count_conf_weighted\", \"area\", \"area_conf_weighted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all aggregation methods\n",
    "agg_eval = pd.DataFrame()\n",
    "agg_scores = pd.DataFrame()\n",
    "agg_methods = [\"area\", \n",
    "            \"area_conf_weighted\", \n",
    "            \"class_count\", \n",
    "            \"class_count_conf_weighted\"\n",
    "            ]\n",
    "for agg_method in agg_methods:\n",
    "    # Pivot table for regression\n",
    "    class_area_per_patient_pivot = patient_lvl.pivot(index=\"Patient ID\", columns=\"class_name\", values=agg_method).reset_index()\n",
    "    class_area_per_patient_pivot = pd.merge(class_area_per_patient_pivot, patient_lvl[[\"Patient ID\" ,\"COVID19\", \"manual_severity_score\"]], on=\"Patient ID\")\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.drop_duplicates().reset_index(drop=True)\n",
    "    # Shuffle\n",
    "    class_area_per_patient_pivot = class_area_per_patient_pivot.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # Use all classes for the model\n",
    "    X = class_area_per_patient_pivot[class_names]\n",
    "    y = class_area_per_patient_pivot[\"manual_severity_score\"]\n",
    "    # groups = class_area_per_patient_pivot[\"Patient ID\"]\n",
    "\n",
    "    # Train models\n",
    "    scores, scores_mean = Regressor.train_models(X, y, \n",
    "                                                # groups=groups, \n",
    "                                                plot=False)\n",
    "    \n",
    "    # Add index level to df for aggregation\n",
    "    scores = pd.concat([scores], keys=[agg_method], names=['Aggregation method'])\n",
    "    scores_mean = pd.concat([scores_mean], keys=[agg_method], names=['Aggregation method'])\n",
    "    \n",
    "    # Add to dataframe\n",
    "    agg_scores = pd.concat([agg_scores, scores])\n",
    "    agg_eval = pd.concat([agg_eval, scores_mean])\n",
    "\n",
    "agg_eval.style.apply(highlight_max).format(\"{:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, only training on L1 and R1 does not seem to improve the overall model performance however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF CLEAN-ish ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental / 2 things I tried that didn't improve the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model stacking. Does not improve the results significantly\n",
    "2. Transformer based NN for tabular data prediction https://github.com/automl/TabPFN . Not improving the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n",
    "# compare ensemble to each baseline classifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "\n",
    "# get a stacking ensemble of models\n",
    "def get_stacking(models):\n",
    "\t# define the base models\n",
    "\tlevel0 = list()\n",
    "\tfor name, model in models.items():\n",
    "\t\tlevel0.append((name, model))\n",
    "\t# define meta learner model\n",
    "\tlevel1 = DecisionTreeClassifier()\n",
    "\t# define the stacking ensemble\n",
    "\tmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\treturn model\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tmodels['lr'] = LogisticRegression()\n",
    "\tmodels['knn'] = KNeighborsClassifier()\n",
    "\t# models['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['rdf'] = RandomForestClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\t# models['bayes'] = GaussianNB()\n",
    "\tmodels['stacking'] = get_stacking(models)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    "\n",
    "\n",
    "X = class_area_per_video_pivot[class_names]\n",
    "y = class_area_per_video_pivot[\"COVID19\"]\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = models['rdf']\n",
    "clf.fit(X, y)\n",
    "\n",
    "x_values = X.columns[np.argsort(clf.feature_importances_)][::-1][:25]\n",
    "y_values = clf.feature_importances_[np.argsort(clf.feature_importances_)][::-1][:25]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "bar = ax.bar(x_values, y_values)\n",
    "plt.setp(ax.get_xticklabels(), fontsize=14, rotation=45, ha=\"right\")\n",
    "ax.set_title('Random Forest feature importance considering all features', fontsize=16)\n",
    "ax.set_ylabel('Importance in %', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, scores_mean = train_models(X, y, models, cv, eval_metrics)\n",
    "scores_mean.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TabPFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import openml\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier\n",
    "from tabpfn.scripts.decision_boundary import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_ensemble_configurations defines how many estimators are averaged, it is bounded by #features * #classes\n",
    "# more ensemble members are slower, but more accurate\n",
    "classifier = TabPFNClassifier(device='cpu', N_ensemble_configurations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_eval, p_eval = classifier.predict(X_test, return_winning_probability=True)\n",
    "print('Prediction time: ', time.time() - start, 'Accuracy', accuracy_score(y_test, y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(file, type(file))\n",
    "out_table = pd.DataFrame(X_test.copy().astype(str))\n",
    "out_table['prediction'] = [f\"{y_e} (p={p_e:.2f})\" for y_e, p_e in zip(y_eval, p_eval)]\n",
    "out_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "\n",
    "# Plot the training points\n",
    "vfunc = np.vectorize(lambda x : np.where(classifier.classes_ == x)[0])\n",
    "y_train_index = vfunc(y_train)\n",
    "y_train_index = y_train_index == 0\n",
    "\n",
    "ax.scatter(X_train.values[:, 0], X_train.values[:, 1], c=y_train_index, cmap=cm_bright)\n",
    "\n",
    "classifier.fit(X_train.values[:, 0:2], y_train_index)\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    classifier, X_train.values[:, 0:2], alpha=0.6, ax=ax, eps=2.0, grid_resolution=25, response_method=\"predict_proba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pocovid2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b946eb799261426a5ad5f24f4c27986c0f2fe154ec5ad35bd8b05c7d39be50ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
